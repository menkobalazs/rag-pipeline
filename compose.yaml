services:
  ollama:
    # Use the official Ollama image
    image: ollama/ollama:latest
    container_name: ollama
    # Expose the default Ollama port externally (optional, but useful for testing)
    ports:
      - "11434:11434"
    # Mount a volume to persist the models and ChromaDB data
    volumes:
      - ollama_models:/root/.ollama
      # Mount the kmpg/data directory so the 'app' service can access the persistent ChromaDB data
      - ./data:/app/data

  model_puller:
    # A lightweight image to run the pull command
    image: alpine/curl:latest
    # This service will wait until the ollama service is running and then pull the required model.
    # The 'depends_on' ensures the ollama container is started.
    # The 'service_healthy' waits for a healthcheck (which is implicitly available for Ollama)
    # The 'command' uses curl to interact with the Ollama API running on the 'ollama' service's host.
    command: >
      /bin/sh -c '
      /usr/bin/curl -X POST http://ollama:11434/api/pull -d "{\"name\": \"llama3.2\"}" &&
      echo "Model llama3.2 pulled successfully."
      '
    depends_on:
      ollama:
        condition: service_started
    # The service stops after running the command
    restart: "no"
    networks:
      - default

  app:
    # Build the image from the local Dockerfile
    build:
      context: .
      dockerfile: Dockerfile
    container_name: kmpg_app
    # Ensure the model is pulled before trying to run the app
    depends_on:
      model_puller:
        condition: service_completed_successfully
    # Map the Streamlit port to the host
    ports:
      - "8501:8501"
    # Mount the 'data' folder to persist the ChromaDB contents
    volumes:
      - ./data:/app/data
    # Set the Ollama URL environment variable for robustness (though the code now uses `base_url` directly)
    environment:
      # Optional: set an environment variable if needed, but the code already uses 'http://ollama:11434'
      OLLAMA_HOST: "http://ollama:11434"
    # Add a healthcheck to ensure the app is ready for use (optional but recommended)
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8501"]
      interval: 30s
      timeout: 10s
      retries: 5

volumes:
  # Volume to persist Ollama models and data
  ollama_models:

networks:
  # Use the default network for inter-service communication
  default:
    driver: bridge